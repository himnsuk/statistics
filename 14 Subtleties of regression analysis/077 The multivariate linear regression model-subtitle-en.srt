1
00:00:00,560 --> 00:00:01,380
OK.

2
00:00:01,520 --> 00:00:09,170
Excellent so far we learned about the simple linear regression we predicted the dependent variable y

3
00:00:09,260 --> 00:00:11,930
with a single regressors x.

4
00:00:11,930 --> 00:00:13,340
It is time to level up.

5
00:00:13,340 --> 00:00:21,710
Now we said that in real life a person's income does not depend solely on education but also their experience

6
00:00:21,980 --> 00:00:25,610
the time they have spent with their current employer and so on.

7
00:00:25,790 --> 00:00:32,140
House prices do not depend on solely their size but also on location and year of construction.

8
00:00:32,330 --> 00:00:38,720
College GPA cannot be predicted solely by a student's S.A.T. score but also by their high school GPA

9
00:00:39,050 --> 00:00:41,190
income gender and so on.

10
00:00:42,460 --> 00:00:48,970
If we want to make good models we need multi-variate regressions multi-variate regressions address the

11
00:00:48,970 --> 00:00:54,850
higher complexity of problems the more variables you have the more factors you are considering in a

12
00:00:54,850 --> 00:00:55,850
model.

13
00:00:56,140 --> 00:01:02,480
In the real world things depend on two three or even 10 or 20 factors.

14
00:01:02,660 --> 00:01:07,960
All right so this is the population multivariate regression model.

15
00:01:07,990 --> 00:01:10,490
It is similar to a simple regression.

16
00:01:10,510 --> 00:01:17,190
The main difference is there are a bunch of independent variables not just one what we are interested

17
00:01:17,190 --> 00:01:20,280
in is the multivariate regression equation.

18
00:01:20,280 --> 00:01:24,060
We want to plug in numbers and predict outcomes right.

19
00:01:24,210 --> 00:01:30,310
Once again why had is the inferred value and 0 is the int..

20
00:01:30,360 --> 00:01:36,880
The independent variables range from x 1 to x K be 1 to be k.

21
00:01:36,890 --> 00:01:39,710
Are there corresponding coefficients.

22
00:01:39,720 --> 00:01:45,630
The last thing to say about multivariate regressions is that it's not about the best fitting line anymore.

23
00:01:45,720 --> 00:01:51,450
Actually it stops being two dimensional and when we have over three dimensions there is no visual way

24
00:01:51,450 --> 00:01:53,480
to represent the data.

25
00:01:53,690 --> 00:01:57,480
So if it is not about the line what is it about.

26
00:01:57,890 --> 00:01:58,600
It's a bell.

27
00:01:58,640 --> 00:02:02,260
The best fitting model as we saw from the OS.

28
00:02:02,300 --> 00:02:06,100
What we really want is the least sum of squared errors.

29
00:02:06,180 --> 00:02:13,940
All right so how do we decrease the models error well by increasing the explanatory power of the model.

30
00:02:14,300 --> 00:02:17,940
Ss e n ss are communicating vessels.

31
00:02:17,950 --> 00:02:20,620
Remember each time we lower one.

32
00:02:20,620 --> 00:02:24,220
The other goes higher with each additional variable.

33
00:02:24,280 --> 00:02:28,400
We increase the explanatory power by zero or more than zero.

34
00:02:28,420 --> 00:02:33,450
We cannot lower it more variables usually equal a better fitting model.

35
00:02:34,640 --> 00:02:40,130
In the next lesson we will see how to determine the optimal number of variables to use.

36
00:02:40,530 --> 00:02:41,580
Thanks for watching.

