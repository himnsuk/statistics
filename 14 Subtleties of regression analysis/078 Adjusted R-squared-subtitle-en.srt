1
00:00:00,870 --> 00:00:06,210
Remember when we talked about R-squared and I told you it was a very widely used measure of explanatory

2
00:00:06,210 --> 00:00:06,920
power.

3
00:00:07,200 --> 00:00:09,680
Well that was almost true.

4
00:00:09,720 --> 00:00:14,980
We'll have to refine this measure its new version will be called the adjusted R-squared.

5
00:00:15,180 --> 00:00:20,220
A statistician would always look at it when performing regression analysis.

6
00:00:20,230 --> 00:00:22,410
So what does it adjust for.

7
00:00:22,450 --> 00:00:29,370
Let's consider two statements we saw in the previous lessons one the R-squared measures how much of

8
00:00:29,370 --> 00:00:36,180
the total variability is explained by our model and to multivariate regressions are always better than

9
00:00:36,280 --> 00:00:41,790
univariate as with each additional variable you add the explanatory power may only increase or stay

10
00:00:41,790 --> 00:00:47,250
the same well be adjusted r squared considers exactly that.

11
00:00:47,250 --> 00:00:51,250
It measures how much of the total variability is explained by our model.

12
00:00:51,300 --> 00:00:58,230
Considering the number of variables the adjusted r squared is always smaller than the R squared as it

13
00:00:58,230 --> 00:01:01,530
penalizes excessive use of variables.

14
00:01:01,530 --> 00:01:04,070
Think about our S.A.T. scores example.

15
00:01:04,200 --> 00:01:10,890
It had an R squared of 0.4 06 and an adjusted R-squared of 0.3 9:9.

16
00:01:10,890 --> 00:01:15,690
Now let's add another variable called random 1 to 3.

17
00:01:15,750 --> 00:01:20,920
I've generated a variable that assigns one two or three randomly to each student.

18
00:01:20,940 --> 00:01:25,620
We are 100 percent sure this variable cannot predict college GPA.

19
00:01:25,620 --> 00:01:34,740
So this is our new model college GPA is equal to be zero plus be 1 times S.A.T. score plus B two times

20
00:01:34,740 --> 00:01:36,850
the random variable.

21
00:01:36,870 --> 00:01:40,320
Let's check out the newly created regression tables.

22
00:01:40,320 --> 00:01:43,880
We can see the new R-Squared is 0.4 0 7.

23
00:01:43,920 --> 00:01:50,220
So it seems we have increased the explanatory power of the model but then our enthusiasm is dampened

24
00:01:50,220 --> 00:01:53,680
by the adjusted R-squared of 0.3 9 3.

25
00:01:53,790 --> 00:01:58,800
We were penalized for adding an additional variable that had no strong explanatory power.

26
00:01:58,830 --> 00:02:02,190
We have added information but have lost value.

27
00:02:02,190 --> 00:02:06,490
The point is you should cherry pick your data to exclude useless information.

28
00:02:07,430 --> 00:02:12,780
However one would assume that regression analysis is smarter than that right.

29
00:02:12,890 --> 00:02:18,660
Adding an impractical variable should be pointed out by the model Well that's true.

30
00:02:18,670 --> 00:02:25,060
Look at the coefficients table we have determined a coefficient for the random 1 to 3 variable but its

31
00:02:25,060 --> 00:02:28,410
p value is 0.6 7:7.

32
00:02:28,480 --> 00:02:33,530
Remember the null hypothesis of the test Beta 2 equals zero.

33
00:02:33,610 --> 00:02:38,050
We cannot reject the no hypothesis at the 68 significance level.

34
00:02:38,050 --> 00:02:41,280
This is an incredibly high p value.

35
00:02:41,320 --> 00:02:47,170
Let me remind you that for a call efficient to be statistically significant We want a p value of less

36
00:02:47,170 --> 00:02:49,070
than 0.05.

37
00:02:49,450 --> 00:02:54,880
Our conclusion is that the variable random one to three not only worsens the explanatory power of the

38
00:02:54,880 --> 00:03:00,460
model reflected by a lower adjusted r squared but is also insignificant.

39
00:03:00,520 --> 00:03:03,310
Therefore it should be dropped altogether.

40
00:03:04,290 --> 00:03:07,040
Dropping useless variables is important.

41
00:03:07,110 --> 00:03:13,770
You can see the original model change from Y had equal zero point to for 7 plus zero point zero zero

42
00:03:13,800 --> 00:03:26,420
two times x 1.0 point 0 1 2 times x 2 2 y had equal zero point 4 8 0 3 plus 0.00 1 5 times x 1.

43
00:03:26,510 --> 00:03:30,860
The choice of the third variable drastically affected our other predictors.

44
00:03:30,920 --> 00:03:36,050
Whenever you have one variable ruining the model you should not use this model altogether because the

45
00:03:36,050 --> 00:03:40,940
bias of this variable is reflected into the coefficients of the other variables.

46
00:03:40,940 --> 00:03:44,650
The correct approach is to remove it from the regression and run a new one.

47
00:03:44,660 --> 00:03:50,890
Omitting the problematic predicter there is one more consideration concerning the removal of variables

48
00:03:50,890 --> 00:03:51,970
from a model.

49
00:03:52,120 --> 00:03:58,410
We can add 100 variables to a model and probably the predictive power of the model will be outstanding.

50
00:03:58,420 --> 00:04:02,440
However this strategy makes regression analysis futile.

51
00:04:02,440 --> 00:04:07,620
You are trying to use a few independent variables that approximately predict the result.

52
00:04:07,630 --> 00:04:14,130
The tradeoff is complex but simplicity is better rewarded than higher explanatory power.

53
00:04:14,140 --> 00:04:19,590
Finally the adjusted r squared is the basis for comparing regression models.

54
00:04:19,600 --> 00:04:25,330
Once again it only makes sense to compare two models considering the same dependent variable and using

55
00:04:25,330 --> 00:04:26,790
the same dataset.

56
00:04:26,830 --> 00:04:32,970
If we compare two models that are about two dependent variables we will make an apples to oranges comparison.

57
00:04:33,100 --> 00:04:40,580
If we use different data sets it's an apples to dinosaur's problem as you can see adjusted r squared

58
00:04:40,580 --> 00:04:44,730
is a step in the right direction but should not be the only measure trusted.

59
00:04:44,900 --> 00:04:46,140
Caution is advised.

60
00:04:46,190 --> 00:04:51,590
Whereas thorough logic and diligence are mandatory in our next lesson.

61
00:04:51,590 --> 00:04:55,250
We will learn how to assess the overall significance of a model.

62
00:04:55,250 --> 00:04:56,270
Thanks for watching.

